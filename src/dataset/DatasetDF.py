import gc
import os
from typing import AnyStr, Dict, Union

import glob2
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

from src.dataset.Transforms import Transforms
from src.settings import settings


class DatasetDF():
    csv_data = Transforms.csv_data


    #@profile
    def __init__(self,
                 test_train   = 'train',
                 data_id: Union[str,int] = '0',
                 size         = -1,
                 fraction     =  1,
                 split: float = 0.1,
                 Y_field      = None,
                 shuffle      = True,
                 transform_X_args = {},
                 transform_Y_args = {},
        ):
        gc.collect()

        self.test_train = test_train
        self.data_id    = data_id
        self.Y_field    = Y_field
        self.split      = split    if self.test_train is 'train' else 0
        self.shuffle    = shuffle  if self.test_train is 'train' else False
        self.fraction   = fraction if self.test_train is 'train' else 1
        self.size       = size

        self.parquet_filenames = sorted(glob2.glob(f"{settings['dir']['data']}/{test_train}_image_data_{data_id}.parquet"))

        self.X:  Dict[AnyStr, np.ndarray]               = { "train": np.ndarray((0,)), "valid": np.ndarray((0,)) }
        self.Y:  Dict[AnyStr, Union[pd.DataFrame,Dict]] = { "train": pd.DataFrame(),   "valid": pd.DataFrame()   }
        self.ID: Dict[AnyStr, np.ndarray]               = { "train": np.ndarray((0,)), "valid": np.ndarray((0,)) }
        for parquet_filename in self.parquet_filenames:
            raw = {
                'train': pd.read_parquet(parquet_filename),
                'valid': None
            }
            # Use size=1 to create a reference dataframe with valid .input_size() + .output_size()
            if self.size > 0:
                raw['valid'] = raw['train'][size+1:size*2]
                raw['train'] = raw['train'][:size]
            else:
                if self.fraction < 1:
                    raw['train'], discard      = train_test_split(raw['train'], train_size=self.fraction, shuffle=self.shuffle)
                    del discard
                if self.split != 0:
                    raw['train'], raw['valid'] = train_test_split(raw['train'], test_size=self.split,     shuffle=self.shuffle, random_state=0)

            if raw['valid'] is None:
                raw['valid'] = pd.DataFrame(columns=raw['train'].columns)

            # Attempt to save memory by doing transform_X() within the loop
            # X can be transformed before np.concatenate, but multi-output Y must be done after pd.concat()
            for key, value in raw.items():
                X = Transforms.transform_X(value, **transform_X_args)
                if len(self.X[key]) == 0: self.X[key] = X
                else:                     self.X[key] = np.concatenate([ self.X[key], X ])
                self.Y[key]  = pd.concat([      self.Y[key],  value[['image_id']]       ])
                self.ID[key] = np.concatenate([ self.ID[key], value['image_id'].values  ])
            del raw; gc.collect()

        self.Y = {
            key: Transforms.transform_Y(value, **transform_Y_args)
            for key,value in self.Y.items()
        }
        pass


    def epoch_size(self):
        return self.X['train'].shape[0]


    def input_shape(self):
        return self.X['train'].shape[1:]  # == (137, 236, 1) / 2


    @classmethod
    def output_shape(cls, Y_field=None):
        if isinstance(Y_field, str):
            return cls.csv_data[Y_field].nunique()

        csv_data     = cls.csv_data[Y_field] if Y_field else cls.csv_data
        output_shape = (csv_data.drop(columns='image_id', errors='ignore')
                                .nunique()
                                .to_dict())
        return output_shape




if __name__ == '__main__' and not os.environ.get('KAGGLE_KERNEL_RUN_TYPE'):
    ### NOTE: loading all datasets at once exceeds 12GB RAM and crashes Python (on 16GB RAM machine)
    ### Runtime: 3m 12s - for in range(0,4)
    ### $ find ./src/ -name '*.py' | xargs perl -p -i -e 's/#@profile/@profile/'
    ### $ time python3 -m memory_profiler src/dataset/DatasetDF.py | less
    #@profile()
    def main():
        for data_id in range(0,4):
            for test_train in ['test', 'train']:
                dataset = DatasetDF(test_train=test_train, data_id=data_id, fraction=1)
                Y_shape = {}
                for key, Y in dataset.Y.items():
                    if isinstance(Y, dict): Y_shape[key] = { k:v.shape for k,v in Y.items() }
                    else:                   Y_shape[key] = Y.shape

                print(f"{test_train}:{data_id} dataset.image_filenames", dataset.parquet_filenames)
                print(f"{test_train}:{data_id} dataset.X",               { key: df.shape for key, df in dataset.X.items() })
                print(f"{test_train}:{data_id} dataset.Y", Y_shape)
                print(f"{test_train}:{data_id} dataset.input_shape()",   dataset.input_shape())
                print(f"{test_train}:{data_id} dataset.output_shape()",  dataset.output_shape())
                print(f"{test_train}:{data_id} dataset.epoch_size()",    dataset.epoch_size())
    main()