{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Parquet Files - RAM/CPU Optimization \n",
    "\n",
    "The Bengali AI dataset is used to explore the different methods available for reading Parquet files (pandas + pyarrow).\n",
    "\n",
    "A common source of trouble for Kernel Only Compeitions, is Out-Of-Memory errors, and the 120 minute submission time limit.\n",
    "\n",
    "This notebook contains:\n",
    "- Syntax and performance for reading Parquet via both Pandas and Pyarrow\n",
    "- Kaggle Kernel RAM/CPU allocation\n",
    "  - 18G RAM\n",
    "  - 2x Intel(R) Xeon(R) CPU @ 2.00GHz CPU\n",
    "- RAM optimized generator function around `pandas.read_parquet()`\n",
    "  - trade 50% RAM (1700MB -> 780MB) for 2x disk IO time (5.8min -> 10.2min runtime) \n",
    "- RAM/CPU profiling of implict dataframe dtype casting \n",
    "  - beware of implicit cast between `unit8` -> `float64` = 8x memory usage\n",
    "  - `skimage.measure.block_reduce(train, (1,2,2,1), func=np.mean, cval=0)` can downsample images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAM/CPU Available In Kaggle Kernel\n",
    "\n",
    "In theory there is 18GB of Kaggle RAM, but loading the entire dataset at once often causes out of memory errors, and doesn't leave anything for the tensorflow model. In practice, datasets need to be loaded one file at a time (or even 75% of a file) to permit a successful compile and submission run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:            18G        898M         17G        928K        757M         17G\r\n",
      "Swap:            0B          0B          0B\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2x Intel(R) Xeon(R) CPU @ 2.00GHz CPU\n",
    "\n",
    "In theory this might allow for optimizations using `pathos.multiprocessing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processor\t: 0\r\n",
      "vendor_id\t: GenuineIntel\r\n",
      "cpu family\t: 6\r\n",
      "model\t\t: 85\r\n",
      "model name\t: Intel(R) Xeon(R) CPU @ 2.00GHz\r\n",
      "stepping\t: 3\r\n",
      "microcode\t: 0x1\r\n",
      "cpu MHz\t\t: 2000.160\r\n",
      "cache size\t: 39424 KB\r\n",
      "physical id\t: 0\r\n",
      "siblings\t: 4\r\n",
      "core id\t\t: 0\r\n",
      "cpu cores\t: 2\r\n",
      "apicid\t\t: 0\r\n",
      "initial apicid\t: 0\r\n",
      "fpu\t\t: yes\r\n",
      "fpu_exception\t: yes\r\n",
      "cpuid level\t: 13\r\n",
      "wp\t\t: yes\r\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\r\n",
      "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa itlb_multihit\r\n",
      "bogomips\t: 4000.32\r\n",
      "clflush size\t: 64\r\n",
      "cache_alignment\t: 64\r\n",
      "address sizes\t: 46 bits physical, 48 bits virtual\r\n",
      "power management:\r\n",
      "\r\n",
      "processor\t: 1\r\n",
      "vendor_id\t: GenuineIntel\r\n",
      "cpu family\t: 6\r\n",
      "model\t\t: 85\r\n",
      "model name\t: Intel(R) Xeon(R) CPU @ 2.00GHz\r\n",
      "stepping\t: 3\r\n",
      "microcode\t: 0x1\r\n",
      "cpu MHz\t\t: 2000.160\r\n",
      "cache size\t: 39424 KB\r\n",
      "physical id\t: 0\r\n",
      "siblings\t: 4\r\n",
      "core id\t\t: 1\r\n",
      "cpu cores\t: 2\r\n",
      "apicid\t\t: 2\r\n",
      "initial apicid\t: 2\r\n",
      "fpu\t\t: yes\r\n",
      "fpu_exception\t: yes\r\n",
      "cpuid level\t: 13\r\n",
      "wp\t\t: yes\r\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\r\n",
      "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa itlb_multihit\r\n",
      "bogomips\t: 4000.32\r\n",
      "clflush size\t: 64\r\n",
      "cache_alignment\t: 64\r\n",
      "address sizes\t: 46 bits physical, 48 bits virtual\r\n",
      "power management:\r\n",
      "\r\n",
      "processor\t: 2\r\n",
      "vendor_id\t: GenuineIntel\r\n",
      "cpu family\t: 6\r\n",
      "model\t\t: 85\r\n",
      "model name\t: Intel(R) Xeon(R) CPU @ 2.00GHz\r\n",
      "stepping\t: 3\r\n",
      "microcode\t: 0x1\r\n",
      "cpu MHz\t\t: 2000.160\r\n",
      "cache size\t: 39424 KB\r\n",
      "physical id\t: 0\r\n",
      "siblings\t: 4\r\n",
      "core id\t\t: 0\r\n",
      "cpu cores\t: 2\r\n",
      "apicid\t\t: 1\r\n",
      "initial apicid\t: 1\r\n",
      "fpu\t\t: yes\r\n",
      "fpu_exception\t: yes\r\n",
      "cpuid level\t: 13\r\n",
      "wp\t\t: yes\r\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\r\n",
      "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa itlb_multihit\r\n",
      "bogomips\t: 4000.32\r\n",
      "clflush size\t: 64\r\n",
      "cache_alignment\t: 64\r\n",
      "address sizes\t: 46 bits physical, 48 bits virtual\r\n",
      "power management:\r\n",
      "\r\n",
      "processor\t: 3\r\n",
      "vendor_id\t: GenuineIntel\r\n",
      "cpu family\t: 6\r\n",
      "model\t\t: 85\r\n",
      "model name\t: Intel(R) Xeon(R) CPU @ 2.00GHz\r\n",
      "stepping\t: 3\r\n",
      "microcode\t: 0x1\r\n",
      "cpu MHz\t\t: 2000.160\r\n",
      "cache size\t: 39424 KB\r\n",
      "physical id\t: 0\r\n",
      "siblings\t: 4\r\n",
      "core id\t\t: 1\r\n",
      "cpu cores\t: 2\r\n",
      "apicid\t\t: 3\r\n",
      "initial apicid\t: 3\r\n",
      "fpu\t\t: yes\r\n",
      "fpu_exception\t: yes\r\n",
      "cpuid level\t: 13\r\n",
      "wp\t\t: yes\r\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\r\n",
      "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa itlb_multihit\r\n",
      "bogomips\t: 4000.32\r\n",
      "clflush size\t: 64\r\n",
      "cache_alignment\t: 64\r\n",
      "address sizes\t: 46 bits physical, 48 bits virtual\r\n",
      "power management:\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Available Libaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `pandas` and `pyarrow` are the two possible libaries to use\n",
    "\n",
    "NOTE: `parquet` and `fastparquet` are not in the Kaggle pip repo, even with the latest available docket images. Whilst these can be obtained via `!pip install parquet fastparquet`, this requires an internet connection which is not allowed for Kernel only competitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'parquet'\n",
      "No module named 'fastparquet'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow\n",
    "import pyarrow.parquet as pq\n",
    "from pyarrow.parquet import ParquetFile\n",
    "\n",
    "try:   import parquet\n",
    "except Exception as exception: print(exception)\n",
    "    \n",
    "try:   import fastparquet\n",
    "except Exception as exception: print(exception)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow\n",
    "import glob2\n",
    "import gc\n",
    "import time\n",
    "import sys\n",
    "import humanize\n",
    "import math\n",
    "import time\n",
    "import psutil\n",
    "import gc\n",
    "import simplejson\n",
    "import skimage\n",
    "import skimage.measure\n",
    "from timeit import timeit\n",
    "from time import sleep\n",
    "from pyarrow.parquet import ParquetFile\n",
    "import pyarrow\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "pd.set_option('display.max_columns',   500)\n",
    "pd.set_option('display.max_colwidth',   -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Parquet via Pandas\n",
    "\n",
    "Pandas is the simplest and recommended option\n",
    "- it takes 40s seconds to physically read all the data\n",
    "- pandas dataset is 6.5GB in RAM. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.6 :: Anaconda, Inc.\r\n"
     ]
    }
   ],
   "source": [
    "!python --version  # Python 3.6.6 :: Anaconda, Inc == original + latest docker (2020-03-14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.25.3'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__  # 0.25.3 == original + latest docker (2020-03-14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../input/bengaliai-cv19/train_image_data_0.parquet',\n",
       " '../input/bengaliai-cv19/train_image_data_1.parquet',\n",
       " '../input/bengaliai-cv19/train_image_data_2.parquet',\n",
       " '../input/bengaliai-cv19/train_image_data_3.parquet']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = sorted(glob2.glob('../input/bengaliai-cv19/train_image_data_*.parquet')); filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single file:\n",
      "sys.getsizeof total 1.6 GB\n",
      "memory total 1.8 GB +system 987.4 MB\n",
      "time:  22.754446029663086\n",
      "----------\n",
      "pd.concat() all files:\n",
      "sys.getsizeof total 6.5 GB\n",
      "memory total 6.6 GB +system 1.1 GB\n",
      "time:  66.73364019393921\n"
     ]
    }
   ],
   "source": [
    "def read_parquet_via_pandas(files=4, cast='uint8', resize=1):\n",
    "    gc.collect(); sleep(5);  # wait for gc to complete\n",
    "    memory_before = psutil.virtual_memory()[3]\n",
    "    # NOTE: loading all the files into a list variable, then applying pd.concat() into a second variable, uses double the memory\n",
    "    df = pd.concat([ \n",
    "        pd.read_parquet(filename).set_index('image_id', drop=True).astype('uint8')\n",
    "        for filename in filenames[:files] \n",
    "    ])\n",
    "    memory_end= psutil.virtual_memory()[3]        \n",
    "\n",
    "    print( \"sys.getsizeof total\", humanize.naturalsize(sys.getsizeof(df)) )\n",
    "    print( \"memory total\",        humanize.naturalsize(memory_end - memory_before), '+system', humanize.naturalsize(memory_before) )        \n",
    "    return df\n",
    "\n",
    "\n",
    "gc.collect(); sleep(2);  # wait for gc to complete\n",
    "print('single file:')\n",
    "time_start = time.time()\n",
    "read_parquet_via_pandas(files=1); gc.collect()\n",
    "print( \"time: \", time.time() - time_start )\n",
    "print('----------')\n",
    "print('pd.concat() all files:')\n",
    "time_start = time.time()\n",
    "read_parquet_via_pandas(files=4); gc.collect()\n",
    "print( \"time: \", time.time() - time_start )\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Parquet via PyArrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a `ParquetFile` is very quick, and memory efficent. It only creates a pointer to the file, but allows us to read the metadata.\n",
    "\n",
    "However there the dataset only contains a single `row_group`, meaning the file can only be read out as a single chunk (no easy row-by-row streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.16.0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow\n",
    "import pyarrow.parquet as pq\n",
    "from pyarrow.parquet import ParquetFile\n",
    "\n",
    "pyarrow.__version__  # 0.16.0 == original + latest docker (2020-03-14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "",
    "_uuid": ""
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.getsizeof 96 Bytes\n",
      "<pyarrow._parquet.FileMetaData object at 0x7f3f38cd9f98>\n",
      "  created_by: parquet-cpp version 1.4.1-SNAPSHOT\n",
      "  num_columns: 32334\n",
      "  num_rows: 50210\n",
      "  num_row_groups: 1\n",
      "  format_version: 1.0\n",
      "  serialized_size: 6089354\n",
      "time:  1.4875917434692383\n"
     ]
    }
   ],
   "source": [
    "# DOCS: https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetFile.html\n",
    "def read_parquet_via_pyarrow_file():\n",
    "    pqfiles = [ ParquetFile(filename) for filename in filenames ]\n",
    "    print( \"sys.getsizeof\", humanize.naturalsize(sys.getsizeof(pqfiles)) )\n",
    "    for pqfile in pqfiles[0:1]: print(pqfile.metadata)\n",
    "    return pqfiles\n",
    "\n",
    "gc.collect(); sleep(2);  # wait for gc to complete\n",
    "time_start = time.time()\n",
    "read_parquet_via_pyarrow_file(); gc.collect()\n",
    "print( \"time: \", time.time() - time_start )\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a pyarrow.Table is faster than pandas (`28s` vs `45s`), but uses more memory (`7.6GB` vs `6.5GB`) and causes an Out-Of-Memory exception if everything is read at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.getsizeof: 1.9 GB\n",
      "sys.getsizeof: 1.9 GB\n",
      "sys.getsizeof: 1.9 GB\n",
      "sys.getsizeof: 1.9 GB\n",
      "sys.getsizeof total: 7.6 GB\n",
      "classes: [<class 'pyarrow.lib.Table'>, <class 'pyarrow.lib.Table'>, <class 'pyarrow.lib.Table'>, <class 'pyarrow.lib.Table'>]\n",
      "shapes: [(50210, 32334), (50210, 32334), (50210, 32334), (50210, 32334)]\n",
      "time:  29.592018842697144\n"
     ]
    }
   ],
   "source": [
    "# DOCS: https://arrow.apache.org/docs/python/parquet.html\n",
    "# DOCS: https://arrow.apache.org/docs/python/generated/pyarrow.Table.html\n",
    "# NOTE: Attempting to read all tables into memory, causes an out of memory exception\n",
    "def read_parquet_via_pyarrow_table():\n",
    "    shapes  = []\n",
    "    classes = []\n",
    "    sizes   = 0\n",
    "    for filename in filenames:\n",
    "        table = pq.read_table(filename) \n",
    "        shapes.append( table.shape )\n",
    "        classes.append( table.__class__ )\n",
    "        size = sys.getsizeof(table); sizes += size\n",
    "        print(\"sys.getsizeof:\",   humanize.naturalsize(sys.getsizeof(table))  )        \n",
    "    print(\"sys.getsizeof total:\", humanize.naturalsize(sizes) )\n",
    "    print(\"classes:\", classes)\n",
    "    print(\"shapes:\",  shapes)    \n",
    "\n",
    "\n",
    "gc.collect(); sleep(2);  # wait for gc to complete\n",
    "time_start = time.time()\n",
    "read_parquet_via_pyarrow_table(); gc.collect()\n",
    "print( \"time: \", time.time() - time_start )\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A generator can be written around pyarrow, but this still reads the contents of an entire file into memory and this function is really slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:   438.52048230171204\n",
      "count:  393\n",
      "min memory_usage:  4.6 GB\n",
      "max memory_usage:  5.7 GB\n",
      "avg memory_usage:  5.1 GB\n"
     ]
    }
   ],
   "source": [
    "import time, psutil, gc\n",
    "\n",
    "gc.collect(); sleep(2)  # wait for gc to complete\n",
    "mem_before   = psutil.virtual_memory()[3]\n",
    "memory_usage = []\n",
    "\n",
    "def read_parquet_via_pyarrow_table_generator(batch_size=128):\n",
    "    for filename in filenames[0:1]:  # only loop over one file for demonstration purposes\n",
    "        gc.collect(); sleep(1)\n",
    "        for batch in pq.read_table(filename).to_batches(batch_size):\n",
    "            mem_current = psutil.virtual_memory()[3]\n",
    "            memory_usage.append( mem_current - mem_before )\n",
    "            yield batch.to_pandas()\n",
    "  \n",
    "\n",
    "time_start = time.time()\n",
    "count = 0\n",
    "for batch in read_parquet_via_pyarrow_table_generator():\n",
    "    count += 1\n",
    "\n",
    "print( \"time:  \", time.time() - time_start )\n",
    "print( \"count: \", count )\n",
    "print( \"min memory_usage: \", humanize.naturalsize(min(memory_usage))  )\n",
    "print( \"max memory_usage: \", humanize.naturalsize(max(memory_usage))  )\n",
    "print( \"avg memory_usage: \", humanize.naturalsize(np.mean(memory_usage)) )\n",
    "pass    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Batch Generator Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to write a batch generator using pandas. In theory this should save memory, at the expense of disk IO. \n",
    "\n",
    "- Timer show that disk IO increase linarly with the number of filesystem reads. \n",
    "- Memory measurements require `gc.collect(); sleep(1)`, but show that average/min memory reduces linearly with filesystem reads\n",
    "\n",
    "There are 8 files to read (including test files in the submission), so the tradeoffs are as follows:\n",
    "- reads_per_file 1 |  44s * 8 =  5.8min + 1700MB RAM (potentually crashing the kernel)\n",
    "- reads_per_file 2 |  77s * 8 = 10.2min +  781MB RAM (minimum required to solve the memory bottleneck)\n",
    "- reads_per_file 3 | 112s * 8 = 14.9min +  508MB RAM (1/8th of total 120min runtime)\n",
    "- reads_per_file 5 | 183s * 8 = 24.4min +  314MB RAM (1/5th of total 120min runtime)\n",
    "\n",
    "This is a memory/time tradeoff, but demonstrates a practical solution to out-of-memory errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reads_per_file 1 | time 46 s | count 1572 | memory {'min': '-1277952 Bytes', 'max': '1.7 GB', 'avg': '821.7 MB', '+system': '4.9 GB'}\n",
      "reads_per_file 2 | time 82 s | count 1576 | memory {'min': '-352256 Bytes', 'max': '514.7 MB', 'avg': '46.3 MB', '+system': '4.9 GB'}\n",
      "reads_per_file 3 | time 116 s | count 1572 | memory {'min': '2.3 MB', 'max': '555.6 MB', 'avg': '81.9 MB', '+system': '4.9 GB'}\n",
      "reads_per_file 5 | time 188 s | count 1580 | memory {'min': '10.6 MB', 'max': '809.1 MB', 'avg': '184.5 MB', '+system': '4.9 GB'}\n"
     ]
    }
   ],
   "source": [
    "memory_before = psutil.virtual_memory()[3]\n",
    "memory_usage  = []\n",
    "\n",
    "def read_parquet_via_pandas_generator(batch_size=128, reads_per_file=5):\n",
    "    for filename in filenames:\n",
    "        num_rows    = ParquetFile(filename).metadata.num_rows\n",
    "        cache_size  = math.ceil( num_rows / batch_size / reads_per_file ) * batch_size\n",
    "        batch_count = math.ceil( cache_size / batch_size )\n",
    "        for n_read in range(reads_per_file):\n",
    "            cache = pd.read_parquet(filename).iloc[ cache_size * n_read : cache_size * (n_read+1) ].copy()\n",
    "            gc.collect(); sleep(1);  # sleep(1) is required to allow measurement of the garbage collector\n",
    "            for n_batch in range(batch_count):            \n",
    "                memory_current = psutil.virtual_memory()[3]\n",
    "                memory_usage.append( memory_current - memory_before )                \n",
    "                yield cache[ batch_size * n_batch : batch_size * (n_batch+1) ].copy()\n",
    "\n",
    "                \n",
    "for reads_per_file in [1,2,3,5]: \n",
    "    gc.collect(); sleep(5);  # wait for gc to complete\n",
    "    memory_before = psutil.virtual_memory()[3]\n",
    "    memory_usage  = []\n",
    "    \n",
    "    time_start = time.time()\n",
    "    count = 0\n",
    "    for batch in read_parquet_via_pandas_generator(batch_size=128, reads_per_file=reads_per_file):\n",
    "        count += 1\n",
    "        \n",
    "    print( \"reads_per_file\", reads_per_file, '|', \n",
    "           'time', int(time.time() - time_start),'s', '|', \n",
    "           'count', count,  '|',\n",
    "           'memory', {\n",
    "                \"min\": humanize.naturalsize(min(memory_usage)),\n",
    "                \"max\": humanize.naturalsize(max(memory_usage)),\n",
    "                \"avg\": humanize.naturalsize(np.mean(memory_usage)),\n",
    "                \"+system\": humanize.naturalsize(memory_before),               \n",
    "            }\n",
    "    )\n",
    "pass    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dtypes and Memory Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory useage can vary by an order of magnitude based on the implcit cast dtype. \n",
    "\n",
    "- Raw pixel values are read from the parquet file as `uint8`\n",
    "- `/ 255.0` or `skimage.measure.block_reduce()` will do an implict cast of `int` -> `float64`\n",
    "- `float64` results in a datastructure 8x as large as `uint8` (`13.0 GB` vs `1.8 GB`)\n",
    "  - This can be avoided by doing an explict cast to `float16` (`3.3 GB`)\n",
    "- `skimage.measure.block_reduce(df, (1,n,n,1), func=np.mean, cval=0)` == `AveragePooling2D(n)` \n",
    "  - reduces data structure memory by `n^2` \n",
    "\n",
    "CPU time: \n",
    "- `float32` (+0.5s) is the fastest cast; `float16` (+8s) is 2x slower than cast `float64` (+4s).\n",
    "- `skimage.measure.block_reduce()` is an expensive operation (3-5x IO read time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype uint8       | uint8   | (50210, 137, 236, 1) |  9.34s | 110.2 MB\n",
      "dtype uint16      | uint16  | (50210, 137, 236, 1) |  10.9s |   3.2 GB\n",
      "dtype uint32      | uint32  | (50210, 137, 236, 1) | 18.05s |   6.5 GB\n",
      "dtype float16     | float16 | (50210, 137, 236, 1) | 57.95s |   3.3 GB\n",
      "dtype float32     | float32 | (50210, 137, 236, 1) | 13.77s |   6.5 GB\n",
      "denoise False     | uint8   | (50210, 137, 236, 1) |   9.7s |  50.7 MB\n",
      "denoise True      | uint8   | (50210, 137, 236, 1) | 11.25s | -6905856 Bytes\n",
      "normalize False   | uint8   | (50210, 137, 236, 1) |  9.31s | 153.7 MB\n",
      "normalize True    | float64 | (50210, 137, 236, 1) | 15.53s |  13.0 GB\n",
      "normalize float16 | float16 | (50210, 137, 236, 1) | 95.19s |   3.3 GB\n",
      "normalize float32 | float32 | (50210, 137, 236, 1) |  15.6s |   6.5 GB\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'resize_fn' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d8bf5211be59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresize\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'float16'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'float32'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'uint8'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mseconds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_single_parquet_via_pandas_with_cast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'resize {resize} {dtype}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mljust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf'| {dtype} | {shape} | {seconds}s | {humanize.naturalsize(memory).rjust(8)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-d8bf5211be59>\u001b[0m in \u001b[0;36mread_single_parquet_via_pandas_with_cast\u001b[0;34m(dtype, normalize, denoise, invert, resize)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# - np.max() produces an enhanced image with thicker lines (maybe slightly easier to read)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# - np.min() produces a  dehanced image with thiner lines (harder to read)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mresize_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresize_fn\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minvert\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mcval\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minvert\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeasure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresize_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'resize_fn' referenced before assignment"
     ]
    }
   ],
   "source": [
    "def read_single_parquet_via_pandas_with_cast(dtype='uint8', normalize=False, denoise=False, invert=True, resize=1):\n",
    "    gc.collect(); sleep(2);\n",
    "    \n",
    "    memory_before = psutil.virtual_memory()[3]\n",
    "    time_start = time.time()        \n",
    "    \n",
    "    train = (pd.read_parquet(filenames[0])\n",
    "               .set_index('image_id', drop=True)\n",
    "               .values.astype(dtype)\n",
    "               .reshape(-1, 137, 236, 1))\n",
    "    \n",
    "    if invert:                                         # Colors | 0 = black      | 255 = white\n",
    "        train = (255-train)                            # invert | 0 = background | 255 = line\n",
    "   \n",
    "    if denoise:                                        # Set small pixel values to background 0\n",
    "        if invert: train *= (train >= 25)              #   0 = background | 255 = line  | np.mean() == 12\n",
    "        else:      train += (255-train)*(train >= 230) # 255 = background |   0 = line  | np.mean() == 244     \n",
    "        \n",
    "    if isinstance(resize, bool) and resize == True:\n",
    "        resize = 2    # Reduce image size by 2x\n",
    "    if resize and resize != 1:                  \n",
    "        # NOTEBOOK: https://www.kaggle.com/jamesmcguigan/bengali-ai-image-processing/\n",
    "        # Out of the different resize functions:\n",
    "        # - np.mean(dtype=uint8) produces produces fragmented images (needs float16 to work properly - but RAM intensive)\n",
    "        # - np.median() produces the most accurate downsampling\n",
    "        # - np.max() produces an enhanced image with thicker lines (maybe slightly easier to read)\n",
    "        # - np.min() produces a  dehanced image with thiner lines (harder to read)\n",
    "        resize_fn = resize_fn or (np.max if invert else np.min)\n",
    "        cval      = 0 if invert else 255\n",
    "        train = skimage.measure.block_reduce(train, (1, resize,resize), cval=cval, func=resize_fn)\n",
    "        \n",
    "    if normalize:\n",
    "        train = train / 255.0          # division casts: int -> float64 \n",
    "\n",
    "\n",
    "    time_end     = time.time()\n",
    "    memory_after = psutil.virtual_memory()[3] \n",
    "    return ( \n",
    "        str(round(time_end - time_start,2)).rjust(5),\n",
    "        # str(sys.getsizeof(train)),\n",
    "        str(memory_after - memory_before).rjust(5), \n",
    "        str(train.shape).ljust(20),\n",
    "        str(train.dtype).ljust(7),\n",
    "    )\n",
    "    \n",
    "\n",
    "gc.collect(); sleep(2);  # wait for gc to complete\n",
    "\n",
    "for dtype in ['uint8', 'uint16', 'uint32', 'float16', 'float32']:  # 'float64' caused OOM error\n",
    "    seconds, memory, shape, dtype = read_single_parquet_via_pandas_with_cast(dtype=dtype)\n",
    "    print(f'dtype {dtype}'.ljust(18) + f'| {dtype} | {shape} | {seconds}s | {humanize.naturalsize(memory).rjust(8)}')\n",
    "\n",
    "for denoise in [False, True]:\n",
    "    seconds, memory, shape, dtype = read_single_parquet_via_pandas_with_cast(denoise=denoise)\n",
    "    print(f'denoise {denoise}'.ljust(18) + f'| {dtype} | {shape} | {seconds}s | {humanize.naturalsize(memory).rjust(8)}')\n",
    "\n",
    "for normalize in [False, True]:\n",
    "    seconds, memory, shape, dtype = read_single_parquet_via_pandas_with_cast(normalize=normalize)\n",
    "    print(f'normalize {normalize}'.ljust(18) + f'| {dtype} | {shape} | {seconds}s | {humanize.naturalsize(memory).rjust(8)}')\n",
    "for dtype in ['float16', 'float32']:\n",
    "    seconds, memory, shape, dtype = read_single_parquet_via_pandas_with_cast(dtype=dtype, normalize=True)\n",
    "    print(f'normalize {dtype}'.ljust(18) + f'| {dtype} | {shape} | {seconds}s | {humanize.naturalsize(memory).rjust(8)}')\n",
    "    \n",
    "# skimage.measure.block_reduce() casts: unit8 -> float64    \n",
    "for resize in [2, 3, 4]:\n",
    "    for dtype in ['float16', 'float32', 'uint8']:\n",
    "        seconds, memory, shape, dtype = read_single_parquet_via_pandas_with_cast(dtype=dtype, resize=resize)\n",
    "        print(f'resize {resize} {dtype}'.ljust(18) + f'| {dtype} | {shape} | {seconds}s | {humanize.naturalsize(memory).rjust(8)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
